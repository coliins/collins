This is my example of live tweets analysis
library(rtweet)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(dplyr)
#place your own appname and unique code here
app_name=""
APIkey=""
APIsecretkey=""
Accesstoken="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
Accesstokensecret="xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

#authenticate via access point
create_token(app = app_name,
             consumer_key = APIkey,
             consumer_secret = APIsecretkey,
             access_token = Accesstoken,
             access_secret = Accesstokensecret)
# sending a tweet from R

post_tweet("Look, i'm tweeting from R in my #kikuyu")# should be 140 cheracters


#SPECIFIC USER ACCOUNT/ORGANIC ACCOUNT
Miguna <- get_timeline("@EstherPassaris", n= 3200)
# Remove retweets from miguna Miguna timeline
Miguna_tweets_organic <- Miguna[Miguna$is_retweet==FALSE, ]

#Removing replies
Miguna_tweets_organic <- subset(Miguna_tweets_organic, is.na(Miguna_tweets_organic$reply_to_status_id))

# RETWEET ACCOUNT ONLY
Miguna_retweets <- Miguna[Miguna$is_retweet==TRUE,]

#REPLIES ACCOUNT ONLY
Miguna_replies <- subset(Miguna, !is.na(Miguna$reply_to_status_id))
# arranginging by the number of retweets and likes for Gates
Miguna_tweets_organic <- Miguna_tweets_organic %>% arrange(-favorite_count)
Miguna_tweets_organic[1,5]
#favourite retweets
Miguna_tweets_organic <- Miguna_tweets_organic %>% arrange(-retweet_count)
Miguna_tweets_organic[1,5]

# Creating a data frame
data <- data.frame(
  category=c("Usertweets", "Retweets", "Replies"),
  count=c(1033, 2136, 13)
)
#VISUALIZATION FOR MIGUNA ACCOUNT
# Adding columns 
library(forestmangr)
data$fraction = data$count / sum(data$count)
data$percentage = data$count / sum(data$count) * 100
data$ymax = cumsum(data$fraction)
data$ymin = c(0, head(data$ymax, n=-1))
# Rounding the data to two decimal points
data <- round_df(data, 2)
# Specify what the legend should say
Type_of_Tweet <- paste(data$category, data$percentage, "%")
ggplot(data, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=Type_of_Tweet)) +
  geom_rect() +
  coord_polar(theta="y") + 
  xlim(c(2, 4)) +
  theme_void() +
  theme(legend.position = "right")+ggtitle("Esther Passaris twitter activity since 2019 dec 13")

#SHOW WHEN THE TWEETS ARE PUBLISHED

colnames(Miguna)[colnames(Miguna)=="screen_name"] <- "Twitter_Account"
ts_plot(dplyr::group_by(Miguna, Twitter_Account), "months") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frequency of Tweets from Esther Passaris by month since Dec 2019",
    subtitle = "Tweet counts aggregated by month",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  )

#SHOW FROM WHERE THE TWEETS ARE PUBLISHED
Miguna_app <- Miguna %>% 
  select(source) %>% 
  group_by(source) %>%
  summarize(count=n())
Miguna_app <- subset(Miguna_app, count > 4)


#VISUALIZATION

data <- data.frame(
  category=Miguna_app$source,
  count=Miguna_app$count
)
data$fraction = data$count / sum(data$count)
data$percentage = data$count / sum(data$count) * 100
data$ymax = cumsum(data$fraction)
data$ymin = c(0, head(data$ymax, n=-1))
data <- round_df(data, 2)
Source <- paste(data$category, data$percentage, "%")
ggplot(data, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=Source)) +
  geom_rect() +
  coord_polar(theta="y") + # Try to remove that to understand how the chart is built initially
  xlim(c(2, 4)) +
  theme_void() +
  theme(legend.position = "right")+ggtitle("platforms where Esther Passaris publishes his tweets")
#SHOW THE MOST FREQUENT WORDS FOUND IN THE TWEETS
Miguna_tweets_organic$text <-  gsub("https\\S*", "", Miguna_tweets_organic$text)
Miguna_tweets_organic$text <-  gsub("@\\S*", "", Miguna_tweets_organic$text) 
Miguna_tweets_organic$text  <-  gsub("amp", "", Miguna_tweets_organic$text) 
Miguna_tweets_organic$text  <-  gsub("[\r\n]", "", Miguna_tweets_organic$text)
Miguna_tweets_organic$text  <-  gsub("[[:punct:]]", "", Miguna_tweets_organic$text)

#REMOVING STOPWORDS
tweets <- Miguna_tweets_organic %>%
  select(text) %>%
  unnest_tokens(word, text)
tweets <- tweets %>%
  anti_join(stop_words)
#VISUALIZATION
tweets %>% # gives you a bar chart of the most frequent words found in the tweets
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n,fill=word)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Most frequent words found in the tweets of Esther Passaris",
       subtitle = "Stop words removed from the list")
#SHOW THE MOST FREQUENTLY USED HASHTAGS
library(wordcloud)
Miguna_tweets_organic$hashtags <- as.character(Miguna_tweets_organic$hashtags)
Miguna_tweets_organic$hashtags <- gsub("c\\(", "", Miguna_tweets_organic$hashtags)
set.seed(1234)
wordcloud(Miguna_tweets_organic$hashtags, min.freq=4, scale=c(3.5, .5), random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

#SHOW THE ACCOUNTS FROM WHICH MOST RETWEETS ORIGINATE
set.seed(1234)
wordcloud(Miguna_retweets$retweet_screen_name, min.freq=5, scale=c(2, .5), random.order=FALSE, rot.per=0.25, 
          colors=brewer.pal(8, "Dark2"))

#PERFORM A SENTIMENT ANALYSIS OF THE TWEETS

library(syuzhet)
# Converting tweets to ASCII to trackle strange characters
tweets <- iconv(tweets, from="UTF-8", to="ASCII", sub="")
# removing retweets, in case needed 
tweets <-gsub("(RT|via)((?:\\b\\w*@\\w+)+)","",tweets)
# removing mentions, in case needed
tweets <-gsub("@\\w+","",tweets)
ew_sentiment<-get_nrc_sentiment((tweets))
sentimentscores<-data.frame(colSums(ew_sentiment[,]))
names(sentimentscores) <- "Score"
sentimentscores <- cbind("sentiment"=rownames(sentimentscores),sentimentscores)
rownames(sentimentscores) <- NULL
ggplot(data=sentimentscores,aes(reorder(x=sentiment,-Score),y=Score))+
  geom_bar(aes(fill=sentiment),stat = "identity")+
  theme(legend.position="none")+
  xlab("Sentiments")+ylab("Scores")+
  ggtitle("Total sentiment based on scores")+
  theme_minimal()+ggtitle("Esther Passaris Tone annalysis since Dec 2019")


#SEARCHING FOR TWEETS
#decide on how you are gping to include the tweets
lockdowneffect <- search_tweets(q="#Magufuli",n=500,lang = "en",
                                include_rts = F)
#TWEETS FROM HASHTAG/TRENDING

tweets.lockdowneffect <- lockdowneffect%>% select(screen_name,text)
head(tweets.lockdowneffect)
#remove https element manually
tweets.lockdowneffect$stripped_text1 <- gsub("https\\S+","",tweets.lockdowneffect$text)
#Use unnest_token to convert to lowercase,remove punctuation and add Id to each tweet
tweets.lockdowneffect_stem <- tweets.lockdowneffect%>%
  select(stripped_text1)%>%unnest_tokens(word,stripped_text1)
#remove stop list from your list of words
cleaned.tweets <- tweets.lockdowneffect_stem%>%
  anti_join(stop_words)
head(cleaned.tweets)
library(tm)
library(stringi)
stri_trans_general(cleaned.tweets, "latin-ascii")
#finding the top ten words used by the users
cleaned.tweets%>% count(word,sort = T)%>%
  top_n(10)%>%mutate(word,reorder(word,n))%>% ggplot(aes(reorder(x=word,-n),y=n,fill=word))+
  geom_col()+xlab(NULL)+theme_classic()+labs(x="count",y="uniquewords")+
                                                          ggtitle("unique word count found in lockdown effect")+coord_flip()+
  geom_text(aes(label=n), position="dodge", vjust=-0.25)

#Now perfom sentinnel analysis
#Perform sentiment analysis using the Bing lexicon and get_sentiments function from the tidytext package
bing_lockdowneffect <- cleaned.tweets%>% inner_join(get_sentiments("bing"))%>%
  count(word,sentiment,sort = T)%>%
  ungroup()
bing_lockdowneffect
# filtering side by side to compare positive vs negative emotion

bing_lockdowneffect%>% group_by(sentiment)%>%top_n(5)%>%ungroup()%>%
  mutate(word=reorder(word,n))%>%ggplot(aes(word,n,fill=sentiment))+
  geom_col(show.legend = F)+facet_wrap(~sentiment,scale="free_y")+
  labs(x=NULL,y="contribution to sentinel")+ggtitle("tweet containing lockdown")+coord_flip()+
  geom_text(aes(label=n), position="dodge", vjust=-0.25)

#Analysing location from tweets
# get a list of unique usernames
unique(lockdowneffect$screen_name)
# how many locations are represented
length(unique(lockdowneffect$location))
tweets.lockdowneffect.loc <- lockdowneffect%>% select(location,text)
head(tweets.lockdowneffect.loc)
tweets.lockdowneffect$stripped_location <- gsub("https\\S+","",tweets.lockdowneffect.loc$location)
tweets.lockdowneffect_stem2 <- tweets.lockdowneffect.loc%>%
  select(location)%>%unnest_tokens(word,location)
cleaned.tweets <- tweets.lockdowneffect_stem2%>%
  anti_join(stop_words)
head(cleaned.tweets)
cleaned.tweets%>% count(word,sort = T)%>%
  top_n(7)%>%mutate(word,reorder(word,n))%>% ggplot(aes(reorder(x=word,-n),y=n,fill=word))+
  geom_col()+xlab(NULL)+theme_classic()+labs(x="count",y="uniquewords")+
  ggtitle("unique location count found in lockdown effect")+coord_flip()+
  geom_text(aes(label=n), position="dodge", vjust=-0.25)

#facebook analysis in R
library(Rook)
library(Rfacebook)
library(dplyr)
library(tidyr)
library(tidytext)
fbOAuth(app_id, app_secret, extended_permissions = TRUE)
require("Rfacebook")
fb_oauth <- fbOAuth(app_id="2810235439074758", app_secret="1eb2b02e84aea15700f52c16acab2128")
#the getUsers function return public information about one or more Facebook user
me <- getUsers("me",token=fb_oauth)
#Connecting to the Facebook API via Authentication Token:
#You can create your token at https://developers.facebook.com/tools/explorer but it only last two hours
token <- 'EAAn75O0UzcYBAHNhltlkXGGwdSxkre414BLH8aFVzda7oQJFV8IPADfZBwRXYFzNKipaCqScUZCSI0cdzuvZAdukwChLP4LJd45eqvMFfAGSRzaBLUejJycOvZBNz318Fj9GvMGn4FsOVdK357f9x2vujDZA5aUOJTjIJXArUdxUZB8BMGFPBm9FP0cWnUcs0T3VpZA8alAeQZDZD'
me <- getUsers("me", token, private_info=TRUE)
